{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import mltools as ml\n",
    "import matplotlib.pyplot as plt # use matplotlib for plotting with inline plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt(\"C:\\\\Users\\\\regin\\\\Desktop\\\\CS 178\\\\Project\\\\X_train.txt\", delimiter = None)\n",
    "Y = np.genfromtxt(\"C:\\\\Users\\\\regin\\\\Desktop\\\\CS 178\\\\Project\\\\Y_train.txt\", delimiter = None)\n",
    "\n",
    "Xte = np.genfromtxt(\"C:\\\\Users\\\\regin\\\\Desktop\\\\CS 178\\\\Project\\\\X_test.txt\", delimiter = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 14) (200000,) (200000, 14)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape, Xte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xva, Ytr, Yva = ml.splitData(X, Y) # Default is 80% train & 20% val\n",
    "Xtr, Ytr = ml.shuffleData(Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 14) (40000, 14)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape, Xva.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking subsample of data so trains faster; Will train on the whole data for Kaggle\n",
    "Xt, Yt = Xtr[:4000], Ytr[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to use predictSoft() function!! For Kaggle, we need to submit probabilites & not just class predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ml.dtree.treeClassify(Xt, Yt, minLeaf = 30, maxDepth = 20)\n",
    "\n",
    "probs = learner.predictSoft(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17346939, 0.82653061],\n",
       "       [0.65      , 0.35      ],\n",
       "       [0.91      , 0.09      ],\n",
       "       ...,\n",
       "       [0.86363636, 0.13636364],\n",
       "       [0.734375  , 0.265625  ],\n",
       "       [0.56976744, 0.43023256]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.7741\n",
      " Validation AUC: 0.6090\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC', learner.auc(Xt,Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', learner.auc(Xva,Yva)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  if x[10] < 239.140000:\n",
      "    if x[10] < 233.535000:\n",
      "      Predict [0.46433471 0.53566529]\n",
      "    else:\n",
      "      Predict [0.61190818 0.38809182]\n",
      "  else:\n",
      "    if x[7] < 3.312850:\n",
      "      Predict [0.75144928 0.24855072]\n",
      "    else:\n",
      "      Predict [0.65255906 0.34744094]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner = ml.dtree.treeClassify()\n",
    "learner.train(Xt, Yt, maxDepth = 2)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 14) (200000, 14) (40000, 14)\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data to avoid numerical data issues (esp for when features are large)\n",
    "XtrP, params = ml.rescale(Xt)\n",
    "XteP, _ = ml.rescale(Xte, params)\n",
    "XvaP, _ = ml.rescale(Xva, params)\n",
    "\n",
    "print(XtrP.shape, XteP.shape, XvaP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5823529  0.4176471 ]\n",
      " [0.85750737 0.14249263]\n",
      " [0.72632018 0.27367982]\n",
      " [0.4984269  0.5015731 ]\n",
      " [0.4691194  0.5308806 ]]\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "learner = ml.linearC.linearClassify()\n",
    "learner.train(XtrP, Yt, initStep = 0.5, stopTol = 1e-6, stopIter = 100)\n",
    "        # When first debugging, can keep stopTol large & stopIter\n",
    "        # but may want to tune for Kaggle\n",
    "probs = learner.predictSoft(XteP)\n",
    "print(probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.6119\n",
      " Validation AUC: 0.6200\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC', learner.auc(XtrP,Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', learner.auc(XvaP,Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ml.nnet.nnetClassify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After constructing classifer, we can define sizes of its layers & initialize values w/ nn.init_weights(self, sizes, init, X, Y)\n",
    "\n",
    "From method description (see mltools/nnet.py)\n",
    "    sizes = [Ninput, N1, N2, ..., Noutput], \n",
    "        Ninput = # of input features\n",
    "        Noutput = # classes\n",
    "    init = {'zeros', 'random'} : initialize to all zeros or random values (breaks symmetry)\n",
    "\n",
    "Train model using gradient descent --> keep track of surrogate loss (here, MSE loss on output vector, compared to 1-of-K representation of class), as well as 0/1 classification loss (error rate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "layers[-1] must equal the number of classes in Y, or 1 for binary Y",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-f6bc3ee26f92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'random'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopTol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstepsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopIter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\CS 178\\Project\\mltools\\nnet.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, Y, init, stepsize, stopTol, stopIter)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# and (self.wts[-1].shape[0]!=1 or len(self.classes)!=2):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layers[-1] must equal the number of classes in Y, or 1 for binary Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: layers[-1] must equal the number of classes in Y, or 1 for binary Y"
     ]
    }
   ],
   "source": [
    "nn.init_weights([14,5,3], 'random', Xt, Yt)\n",
    "nn.train(Xt, Yt, stopTol = 1e-8, stepsize = 0.25, stopIter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1 : Jsur = 0.47627597374835745, J01 = 0.3375\n",
      "it 2 : Jsur = 0.454734257745067, J01 = 0.3375\n",
      "it 4 : Jsur = 0.43878327246468973, J01 = 0.3375\n",
      "it 8 : Jsur = 0.4395518030470107, J01 = 0.3375\n",
      "it 16 : Jsur = 0.43785748858290224, J01 = 0.3375\n",
      "it 32 : Jsur = 0.43796308019416375, J01 = 0.3375\n"
     ]
    }
   ],
   "source": [
    "# Need to specify RIGHT number of input & output layer\n",
    "nn.init_weights([Xt.shape[1], 5, len(np.unique(Yt))], 'random', Xt, Yt)\n",
    "        # One hidden layer w/ 5 weights\n",
    "nn.train(Xt, Yt, stopTol = 1e-8, stepsize = 0.25, stopIter = 50) # Really small stopIter so it will stop fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.5830\n",
      " Validation AUC: 0.5878\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC', nn.auc(Xt,Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(Xva,Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option you can change is the activation function. This is the function that is in the inner layers. By default, the code comes w/ the tanh, but logistic is also coded in & you can specify it.\n",
    "\n",
    "Also, to avoid numerical overflow issues, you can try using the rescaled features as we did w/ linear classifer.\n",
    "\n",
    "You can play with shapes of hidden layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1 : Jsur = 0.42623774042808543, J01 = 0.3265\n",
      "it 2 : Jsur = 0.42137332587258086, J01 = 0.31775\n",
      "it 4 : Jsur = 0.41970865808644636, J01 = 0.31425\n",
      "it 8 : Jsur = 0.4191085786294226, J01 = 0.3135\n",
      "it 16 : Jsur = 0.4189133410877228, J01 = 0.31275\n",
      "it 32 : Jsur = 0.4189198193324197, J01 = 0.313\n",
      "it 64 : Jsur = 0.4188654806217776, J01 = 0.3135\n",
      "      Train AUC: 0.6459\n",
      " Validation AUC: 0.6542\n"
     ]
    }
   ],
   "source": [
    "nn.setActivation('logistic')\n",
    "nn.init_weights([XtrP.shape[1], 20, len(np.unique(Yt))], 'random', XtrP, Yt)\n",
    "nn.train(XtrP, Yt, stopTol = 1e-8, stepsize = 0.25, stopIter = 100)\n",
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC', nn.auc(XtrP,Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(XvaP,Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also provide a custom activation function. Note that for last layer you will probably always want the sigmoid function, so only change those for the inner layers. \n",
    "\n",
    "The function definition is this:\n",
    "    setActivation(self, method, sig = None, d_sig = None, sig_0 = None, d_sig_0 = None) \n",
    "    \n",
    "You can call it with method = 'custom' & then specify both sig & d_sig (the sig_0 and d_sig_0 ones are the last layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a dummy activation method (f(x) = x); just make sure using nonlinearity activation function\n",
    "sig = lambda z: np.atleast_2d(z)\n",
    "dsig = lambda z: np.atleast_2d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1 : Jsur = 0.4366100250381344, J01 = 0.33675\n",
      "it 2 : Jsur = 0.43234981456693744, J01 = 0.336\n",
      "it 4 : Jsur = 0.4264222651302991, J01 = 0.33225\n",
      "it 8 : Jsur = 0.4216383877791734, J01 = 0.325\n",
      "it 16 : Jsur = 0.4190581339163307, J01 = 0.318\n",
      "it 32 : Jsur = 0.4182963680198954, J01 = 0.317\n",
      "it 64 : Jsur = 0.4181871728686476, J01 = 0.31675\n",
      "      Train AUC: 0.6458\n",
      " Validation AUC: 0.6546\n"
     ]
    }
   ],
   "source": [
    "nn = ml.nnet.nnetClassify()\n",
    "nn.init_weights([Xt.shape[1], Xt.shape[1]*2, len(np.unique(Yt))], 'random', XtrP, Yt)\n",
    "\n",
    "nn.setActivation('custom', sig, dsig)\n",
    "\n",
    "nn.train(XtrP, Yt, stopTol = 1e-20, stepsize = 0.25, stopIter = 100)\n",
    "\n",
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC', nn.auc(XtrP,Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(XvaP,Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're learned that one way of guessing how well we're doing with different model parameters is to plot the train & val errors as a function of that parameters (eg. k in KNN or degree in linear classifer/regression).\n",
    "\n",
    "Now it seems like there could be more parameters involved. One example is minLeaf & the maxDepth values in decision tree. \n",
    "\n",
    "When it's two parameter, you can simply use heatmaps. The x & y axes represent the parameters, and the \"heat\" is the validation error as the \"third\" dimensions\n",
    "\n",
    "We're going to use dummy function to show that. Assume we have 2 parameters p1 & p2, and prediction accuracy is p1 + p2 (just as ex, you would use real evaluation metric like AUC in real applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.arange(5)\n",
    "p2 = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.zeros([p1.shape[0], p2.shape[0]])\n",
    "for i in range(p1.shape[0]):\n",
    "    for j in range(p2.shape[0]):\n",
    "        acc[i][j] = p1[i] + p2[j]\n",
    "        # here you would do your prediction & scoring\n",
    "        # store scoring in acc[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [2., 3., 4., 5., 6.],\n",
       "       [3., 4., 5., 6., 7.],\n",
       "       [4., 5., 6., 7., 8.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize = (8,5))\n",
    "\n",
    "heatmap = ax.matshow(acc)\n",
    "f.colorbar(heatmap)\n",
    "\n",
    "ax.set_xticks(p1)\n",
    "ax.set_xticklabels(['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "ax.set_yticks(p2)\n",
    "ax.set_yticklabels(['%d' % p for p in p2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that last classifer we ran was best one (after we ussed all that we know to verify it is best including plot from previous block) Now let's run test data & create file to be submitted.\n",
    "\n",
    "Each line in file is data point ID & probability of P(Y=1). There's also header line. Here's how you can create it simply from prob matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5823529 , 0.4176471 ],\n",
       "       [0.85750737, 0.14249263],\n",
       "       [0.72632018, 0.27367982],\n",
       "       ...,\n",
       "       [0.66034152, 0.33965848],\n",
       "       [0.56398116, 0.43601884],\n",
       "       [0.56502193, 0.43497807]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for submission by taking P(Y=1) column from probs and just add a running index as the first column.\n",
    "Y_sub = np.vstack([np.arange(Xte.shape[0]), probs[:,1]]).T\n",
    "\n",
    "#Specify header (ID, Prob1) and specify comments as '' so header won't be commented out with the # sign\n",
    "np.savetxt(\"C:\\\\Users\\\\regin\\\\Desktop\\\\CS 178\\\\Project\\\\Y_random.txt\", Y_sub, '%d, %.5f', header = 'ID,Prob1', comments = '', delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
